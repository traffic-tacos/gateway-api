# Redis CPU 100% ìµœì í™” ê°€ì´ë“œ

## ğŸ”´ ë¬¸ì œ ìƒí™©

### ì¦ìƒ
- **Redis CPU**: 99.8% (ê±°ì˜ í¬í™”)
- **Gateway API**: `redis: connection pool timeout`, `context deadline exceeded`
- **K8s Pods**: `CrashLoopBackOff` (Redis ì—°ê²° ì‹¤íŒ¨ë¡œ ì¸í•œ ì´ˆê¸°í™” ì‹¤íŒ¨)
- **ì‚¬ìš©ì ì˜í–¥**: 500 Internal Server Error

### ê·¼ë³¸ ì›ì¸
1. **ë†’ì€ Redis ëª…ë ¹ ë¹ˆë„**: Join APIê°€ **7ê°œì˜ ê°œë³„ Redis í˜¸ì¶œ**
   - 10,000 RPS * 7 calls = **70,000 commands/sec**
2. **Redis ê³¼ë¶€í•˜**: CPU 99.8% â†’ ì‘ë‹µ ì§€ì—° â†’ Connection pool exhaustion
3. **ì•…ìˆœí™˜**: ì‹ ê·œ Pod ë¶€íŒ… ì‹¤íŒ¨ â†’ ë” ë§ì€ Pod ìƒì„± ì‹œë„ â†’ Redis ë¶€í•˜ ê°€ì¤‘

---

## âš¡ í•´ê²° ë°©ì•ˆ: 3ë‹¨ê³„ ì „ëµ

### **1ë‹¨ê³„: ì¦‰ì‹œ ì ìš© (ì• í”Œë¦¬ì¼€ì´ì…˜ ë ˆë²¨)** â±ï¸ 1-2ì‹œê°„

#### 1.1 Circuit Breaker íŒ¨í„´ (ìµœìš°ì„  ê¶Œì¥) â­â­â­

**ëª©ì **: Redis ì¥ì•  ì‹œ fallback ë¡œì§ìœ¼ë¡œ ì„œë¹„ìŠ¤ ìœ ì§€

**êµ¬í˜„**:
```go
// internal/middleware/circuit_breaker.go
type CircuitBreaker struct {
    client            redis.UniversalClient
    state             CircuitBreakerState  // CLOSED, OPEN, HALF_OPEN
    failureCount      int
    maxFailures       int           // 5íšŒ ì—°ì† ì‹¤íŒ¨ ì‹œ OPEN
    resetTimeout      time.Duration // 10ì´ˆ í›„ HALF_OPEN ì¬ì‹œë„
    halfOpenSuccesses int           // 3íšŒ ì„±ê³µ ì‹œ CLOSED
}
```

**ì ìš© ì˜ˆì‹œ**:
```go
// Before: Direct Redis call
result, err := q.luaExecutor.EnqueueAtomic(ctx, req.EventID, req.UserID, dedupeKey)

// After: With Circuit Breaker
err := circuitBreaker.Execute(ctx, func() error {
    result, err = q.luaExecutor.EnqueueAtomic(ctx, req.EventID, req.UserID, dedupeKey)
    return err
})

if err != nil {
    // Fallback: Return degraded response
    if circuitBreaker.GetState() == StateOpen {
        return c.Status(503).JSON(ErrorResponse{
            Error: ErrorDetail{
                Code:    "SERVICE_UNAVAILABLE",
                Message: "System is temporarily overloaded. Please retry in 10 seconds.",
            },
        })
    }
}
```

**íš¨ê³¼**:
- âœ… Redis ì¥ì•  ì‹œ **500 â†’ 503** (ë” ì˜ë¯¸ ìˆëŠ” ì—ëŸ¬)
- âœ… Redis ë¶€í•˜ ê°ì†Œ (ì‹¤íŒ¨ ì‹œ ìš”ì²­ ì°¨ë‹¨)
- âœ… **ìë™ ë³µêµ¬** (HALF_OPEN â†’ CLOSED)

---

#### 1.2 Redis Pipeline ìµœì í™” (ì´ë¯¸ ì ìš© ì™„ë£Œ) â­â­â­

**ë¬¸ì œ**: Join APIì—ì„œ 7ê°œì˜ ê°œë³„ Redis í˜¸ì¶œ
```go
// Before: 7 round trips
SET queue:waiting:{token}           // 1
ZADD queue:event:{eventID}         // 2
EXPIRE queue:event:{eventID}       // 3
SET heartbeat:{token}              // 4
ZADD position_index:{eventID}     // 5
EXPIRE position_index:{eventID}   // 6
ZRANK position_index:{eventID}    // 7 (asyncë¡œ ì´ë™)
```

**í•´ê²°ì±…**: Pipelineìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬
```go
pipe := q.redisClient.Pipeline()

// 1. Store queue data
pipe.Set(ctx, queueKey, queueDataBytes, 30*time.Minute)

// 2-3. Add to ZSET with TTL
pipe.ZAdd(ctx, eventQueueKey, redis.Z{Score: score, Member: waitingToken})
pipe.Expire(ctx, eventQueueKey, 1*time.Hour)

// 4. Heartbeat
pipe.Set(ctx, heartbeatKey, "alive", 5*time.Minute)

// 5-6. Position index
pipe.ZAdd(ctx, positionIndexKey, redis.Z{Score: score, Member: waitingToken})
pipe.Expire(ctx, positionIndexKey, 1*time.Hour)

// Execute all in a single round trip
pipe.Exec(ctx)

// 7. ZRANKì„ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ (Join ì‘ë‹µ ë¸”ë¡œí‚¹ ë°©ì§€)
go func() {
    position, _ := q.streamQueue.GetGlobalPosition(bgCtx, req.EventID, req.UserID, result.StreamID)
}()
```

**íš¨ê³¼**:
- âœ… **7 calls â†’ 2 calls** (Lua script + Pipeline)
- âœ… Redis ëª…ë ¹ ìˆ˜: **70,000 â†’ 20,000 commands/sec** (71% ê°ì†Œ)
- âœ… ì˜ˆìƒ Redis CPU: **99.8% â†’ ~30%**

---

#### 1.3 Connection Pool ìµœì í™” â­â­

**í˜„ì¬ ì„¤ì •**:
```yaml
REDIS_POOL_SIZE: "1000"
REDIS_POOL_TIMEOUT: "10s"
REDIS_MIN_IDLE_CONNS: "100"
REDIS_DIAL_TIMEOUT: "10s"
REDIS_READ_TIMEOUT: "5s"
REDIS_CONN_MAX_IDLE_TIME: "4m"
```

**ê¶Œì¥ ì„¤ì •** (ê³ ë¶€í•˜ í™˜ê²½):
```yaml
REDIS_POOL_SIZE: "2000"              # â¬†ï¸ 1000 â†’ 2000
REDIS_POOL_TIMEOUT: "15s"            # â¬†ï¸ 10s â†’ 15s
REDIS_MIN_IDLE_CONNS: "200"          # â¬†ï¸ 100 â†’ 200
REDIS_DIAL_TIMEOUT: "15s"            # â¬†ï¸ 10s â†’ 15s
REDIS_READ_TIMEOUT: "8s"             # â¬†ï¸ 5s â†’ 8s
REDIS_WRITE_TIMEOUT: "8s"            # ğŸ†• ì¶”ê°€
REDIS_MAX_RETRIES: "5"               # â¬†ï¸ 3 â†’ 5
REDIS_MIN_RETRY_BACKOFF: "50ms"      # ğŸ†• ì¶”ê°€
REDIS_MAX_RETRY_BACKOFF: "500ms"     # ğŸ†• ì¶”ê°€
```

**íš¨ê³¼**:
- âœ… Connection pool timeout ê°ì†Œ
- âœ… ì¼ì‹œì  ì§€ì—° í—ˆìš© (retry backoff)
- âœ… Pod ì´ˆê¸°í™” ì•ˆì •ì„± í–¥ìƒ

---

#### 1.4 Graceful Degradation â­

**ì „ëµ**: Redis ì¥ì•  ì‹œ í•„ìˆ˜ ê¸°ëŠ¥ë§Œ ì œê³µ

```go
// Queue Join API fallback
if circuitBreaker.GetState() == StateOpen {
    // Return estimated position without Redis
    return c.Status(fiber.StatusAccepted).JSON(JoinQueueResponse{
        WaitingToken: "fallback_" + uuid.New().String(),
        PositionHint: 10000, // Conservative estimate
        Status:       "waiting",
        Message:      "Queue position approximate (system degraded)",
    })
}
```

**íš¨ê³¼**:
- âœ… **500 ì—ëŸ¬ â†’ ë¶€ë¶„ ê¸°ëŠ¥ ì œê³µ**
- âœ… ì‚¬ìš©ì ê²½í—˜ ê°œì„  (ì™„ì „ ì‹¤íŒ¨ë³´ë‹¤ ë‚˜ìŒ)

---

### **2ë‹¨ê³„: ë‹¨ê¸° ìµœì í™” (ì¸í”„ë¼ ë ˆë²¨)** â±ï¸ 1-2ì¼

#### 2.1 ElastiCache ì˜¤í† ìŠ¤ì¼€ì¼ë§ ê°œì„  â­â­â­

**í˜„ì¬ ë¬¸ì œ**:
- CPU 70% íŠ¸ë¦¬ê±° â†’ ì‹ ê·œ ë…¸ë“œ ì¶”ê°€ê¹Œì§€ **5-10ë¶„**
- ì´ë¯¸ Redisê°€ í¬í™” ìƒíƒœ â†’ **ë„ˆë¬´ ëŠ¦ìŒ**

**í•´ê²°ì±… 1: ë” ê³µê²©ì ì¸ ìŠ¤ì¼€ì¼ë§**

```hcl
# modules/elasticache/main.tf
resource "aws_appautoscaling_policy" "redis_cpu_target_tracking" {
  target_tracking_scaling_policy_configuration {
    predefined_metric_specification {
      predefined_metric_type = "ElastiCachePrimaryEngineCPUUtilization"
    }
    
    target_value       = 50           # âœ… 70% â†’ 50% (ë” ë¹ ë¥¸ íŠ¸ë¦¬ê±°)
    scale_in_cooldown  = 300          # 5ë¶„ - scale in ëŒ€ê¸°
    scale_out_cooldown = 30           # âœ… 60s â†’ 30s (ë” ë¹ ë¥¸ scale out)
  }
}
```

**í•´ê²°ì±… 2: CloudWatch ì•ŒëŒ + Step Scaling (ë³‘ë ¬ ì‹¤í–‰)**

```hcl
# CPU 80% ì´ˆê³¼ ì‹œ ì¦‰ì‹œ +2 ë…¸ë“œ ì¶”ê°€
resource "aws_cloudwatch_metric_alarm" "redis_cpu_high" {
  alarm_name          = "redis-cpu-critical"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1                    # âœ… 1ë¶„ ë‚´ íŒë‹¨
  metric_name         = "EngineCPUUtilization"
  namespace           = "AWS/ElastiCache"
  period              = 60
  statistic           = "Average"
  threshold           = 80
  alarm_actions       = [aws_appautoscaling_policy.redis_step_scaling.arn]
}

resource "aws_appautoscaling_policy" "redis_step_scaling" {
  step_scaling_policy_configuration {
    adjustment_type = "ChangeInCapacity"
    cooldown        = 60
    
    step_adjustment {
      metric_interval_lower_bound = 0
      metric_interval_upper_bound = 10
      scaling_adjustment          = 1  # CPU 80-90% â†’ +1 ë…¸ë“œ
    }
    
    step_adjustment {
      metric_interval_lower_bound = 10
      scaling_adjustment          = 2  # CPU 90%+ â†’ +2 ë…¸ë“œ
    }
  }
}
```

**íš¨ê³¼**:
- âœ… ìŠ¤ì¼€ì¼ ì•„ì›ƒ **10ë¶„ â†’ 2-3ë¶„**
- âœ… ê¸‰ê²©í•œ íŠ¸ë˜í”½ ì¦ê°€ ëŒ€ì‘
- âœ… CPU 50% ê¸°ì¤€ìœ¼ë¡œ ì˜ˆë°©ì  ìŠ¤ì¼€ì¼ë§

---

#### 2.2 Read Replica í™œìš© ê°•í™” â­â­

**í˜„ì¬ ìƒíƒœ**: Read Replica 3ê°œ (ìƒ¤ë“œë‹¹ 1ê°œ)

**ìµœì í™” ì „ëµ**:
1. **Status APIëŠ” Read Replicaë¡œë§Œ ë¼ìš°íŒ…**
   ```go
   // Status APIì—ì„œë§Œ ReadOnly í™œì„±í™”
   if isStatusAPI {
       redisClient = redis.NewClusterClient(&redis.ClusterOptions{
           ReadOnly: true,  // âœ… Replicaë¡œë§Œ ì½ê¸°
       })
   }
   ```

2. **Read Replica ì¶”ê°€** (ìƒ¤ë“œë‹¹ 1ê°œ â†’ 2ê°œ)
   ```hcl
   resource "aws_elasticache_replication_group" "redis_cluster" {
     replicas_per_node_group = 2  # âœ… 1 â†’ 2
   }
   ```

**íš¨ê³¼**:
- âœ… Primary ë…¸ë“œ CPU ë¶€í•˜ ë¶„ì‚° (ì½ê¸° 50% ê°ì†Œ)
- âœ… Status API ì„±ëŠ¥ ê°œì„ 

---

#### 2.3 Pre-warming + Reserved Capacity â­

**ë¬¸ì œ**: ë¶€í•˜ í…ŒìŠ¤íŠ¸ë‚˜ í”¼í¬ ì‹œê°„ ì˜ˆì¸¡ ê°€ëŠ¥

**í•´ê²°ì±…**: CloudWatch Event + Lambdaë¡œ ì˜ˆì¸¡ì  ìŠ¤ì¼€ì¼ë§

```python
# lambda/pre_scale_redis.py
import boto3

def lambda_handler(event, context):
    """
    í‹°ì¼“ ì˜¤í”ˆ 10ë¶„ ì „ Redis ë…¸ë“œ ë¯¸ë¦¬ ì¦ì„¤
    """
    autoscaling = boto3.client('application-autoscaling')
    
    # í˜„ì¬ ë…¸ë“œ ìˆ˜ í™•ì¸
    response = autoscaling.describe_scalable_targets(
        ServiceNamespace='elasticache',
        ResourceIds=['replication-group/traffic-tacos-redis']
    )
    
    current_capacity = response['ScalableTargets'][0]['DesiredCapacity']
    
    # +2 ë…¸ë“œ ë¯¸ë¦¬ ì¶”ê°€
    autoscaling.register_scalable_target(
        ServiceNamespace='elasticache',
        ResourceId='replication-group/traffic-tacos-redis',
        ScalableTargetAction={
            'MinCapacity': current_capacity + 2,
            'MaxCapacity': current_capacity + 2
        }
    )
```

**CloudWatch Event Rule**:
```hcl
resource "aws_cloudwatch_event_rule" "ticket_open_pre_scale" {
  name                = "ticket-open-pre-scale"
  description         = "Scale Redis 10 minutes before ticket open"
  schedule_expression = "cron(50 8 * * ? *)"  # 09:00 ì˜¤í”ˆ â†’ 08:50 ìŠ¤ì¼€ì¼ë§
}
```

**íš¨ê³¼**:
- âœ… í”¼í¬ íƒ€ì„ ì „ **ì˜ˆë°©ì  í™•ì¥**
- âœ… ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì§€ì—° ë¬¸ì œ í•´ê²°

---

### **3ë‹¨ê³„: ì¥ê¸° ì „ëµ (ì•„í‚¤í…ì²˜ ê°œì„ )** â±ï¸ 1-4ì£¼

#### 3.1 ë…¸ë“œ íƒ€ì… ì—…ê·¸ë ˆì´ë“œ â­â­

**í˜„ì¬**: `cache.r7g.large` (2 vCPU, 13.07 GiB)

**ê¶Œì¥**: `cache.r7g.xlarge` (4 vCPU, 26.32 GiB)

**ë¹„ìš© vs ì„±ëŠ¥**:
```
r7g.large:  $0.141/hour * 3 shards * 2 nodes = $0.846/hour
r7g.xlarge: $0.282/hour * 3 shards * 2 nodes = $1.692/hour

ì¶”ê°€ ë¹„ìš©: $0.846/hour = $613/month
```

**íš¨ê³¼**:
- âœ… CPU ì—¬ìœ  2ë°° ì¦ê°€
- âœ… Connection ì²˜ë¦¬ ëŠ¥ë ¥ í–¥ìƒ
- âœ… ì˜¤í† ìŠ¤ì¼€ì¼ë§ ë¹ˆë„ ê°ì†Œ

---

#### 3.2 ìƒ¤ë“œ ìˆ˜ ì¦ê°€ (Re-sharding) â­â­â­

**í˜„ì¬**: 3 shards

**ê¶Œì¥**: 5 shards

**êµ¬í˜„**:
```hcl
resource "aws_elasticache_replication_group" "redis_cluster" {
  num_node_groups = 5  # âœ… 3 â†’ 5
  replicas_per_node_group = 2
}
```

**íš¨ê³¼**:
- âœ… ë°ì´í„° ë¶„ì‚° ê°œì„  (shardë‹¹ ë¶€í•˜ 40% ê°ì†Œ)
- âœ… CPU ë³‘ë ¬ ì²˜ë¦¬ ëŠ¥ë ¥ í–¥ìƒ
- âœ… **ë™ì‹œ ì²˜ë¦¬ëŸ‰ 67% ì¦ê°€** (3 â†’ 5 shards)

**ì£¼ì˜ì‚¬í•­**:
- âš ï¸ Re-sharding ì¤‘ **ì„±ëŠ¥ ì €í•˜** (10-30ë¶„)
- âš ï¸ Hash slot ì¬ë¶„ë°°ë¡œ ì¼ì‹œì  ì§€ì—° ë°œìƒ
- âš ï¸ **í”¼í¬ ì‹œê°„ ì™¸ ì‹¤í–‰ í•„ìˆ˜**

---

#### 3.3 Karpenter-style Proactive Scaling â­

**ê°œë…**: Gateway Pod ì¦ê°€ â†’ Redis ë…¸ë“œë„ ìë™ ì¦ê°€

**êµ¬í˜„**:
```python
# lambda/karpenter_style_redis_scaling.py
def monitor_gateway_pods():
    """
    Gateway Pod ìˆ˜ ëª¨ë‹ˆí„°ë§ â†’ Redis ë…¸ë“œ ì˜ˆì¸¡ì  ì¦ê°€
    """
    eks = boto3.client('eks')
    
    # Gateway Pod ìˆ˜ í™•ì¸
    gateway_pods = get_pod_count(namespace='tacos-app', label='app=gateway-api')
    
    # Pod ìˆ˜ì— ë¹„ë¡€í•œ Redis ë…¸ë“œ ê³„ì‚°
    # 10 Gateway Podsë‹¹ Redis ë…¸ë“œ 1ê°œ
    required_redis_nodes = max(3, gateway_pods // 10)
    
    # Redis ë…¸ë“œ ì¡°ì •
    scale_redis_cluster(required_redis_nodes)
```

**íš¨ê³¼**:
- âœ… Gateway ì¦ê°€ â†’ Redisë„ ë™ì‹œ ì¦ê°€
- âœ… **ì‚¬ì „ ì˜ˆë°©ì  ìŠ¤ì¼€ì¼ë§**

---

## ğŸ“Š ì¢…í•© ë¹„êµ

| ì „ëµ | ì ìš© ì‹œê°„ | ë¹„ìš© | íš¨ê³¼ | ìš°ì„ ìˆœìœ„ |
|---|---|---|---|---|
| **Circuit Breaker** | 1ì‹œê°„ | ë¬´ë£Œ | 500 â†’ 503 | â­â­â­ |
| **Pipeline ìµœì í™”** | ì™„ë£Œ | ë¬´ë£Œ | 70K â†’ 20K cmd/s | â­â­â­ |
| **Connection Pool ì¦ê°€** | 10ë¶„ | ë¬´ë£Œ | Timeout ê°ì†Œ | â­â­â­ |
| **ì˜¤í† ìŠ¤ì¼€ì¼ë§ ê°œì„ ** | 1ì¼ | ë¬´ë£Œ | Scale 10ë¶„ â†’ 2ë¶„ | â­â­â­ |
| **Read Replica ì¶”ê°€** | 1ì¼ | +$300/ì›” | Primary CPU 50% ê°ì†Œ | â­â­ |
| **ë…¸ë“œ íƒ€ì… ì—…ê·¸ë ˆì´ë“œ** | 2ì¼ | +$613/ì›” | CPU ì—¬ìœ  2ë°° | â­â­ |
| **ìƒ¤ë“œ ìˆ˜ ì¦ê°€** | 1ì£¼ | +$1,000/ì›” | ì²˜ë¦¬ëŸ‰ 67% ì¦ê°€ | â­â­â­ |
| **Pre-warming Lambda** | 3ì¼ | ë¬´ë£Œ | ì˜ˆë°©ì  í™•ì¥ | â­â­ |

---

## ğŸ¯ ê¶Œì¥ ì‹¤í–‰ ìˆœì„œ

### **Week 1: ê¸´ê¸‰ ì¡°ì¹˜** (ë¹„ìš© ë¬´ë£Œ)
1. âœ… Pipeline ìµœì í™” ë°°í¬ (ì´ë¯¸ ì™„ë£Œ)
2. â³ Circuit Breaker êµ¬í˜„ ë° ë°°í¬
3. â³ Connection Pool ì„¤ì • ì¡°ì •
4. â³ ElastiCache ì˜¤í† ìŠ¤ì¼€ì¼ë§ ê°œì„  (50%, 30s)

### **Week 2: ë‹¨ê¸° ê°œì„ ** (ë¹„ìš© +$300/ì›”)
5. Read Replica ì¶”ê°€ (ìƒ¤ë“œë‹¹ 2ê°œ)
6. Pre-warming Lambda êµ¬í˜„ (í‹°ì¼“ ì˜¤í”ˆ ì‹œê°„ ê¸°ë°˜)

### **Week 3-4: ì¥ê¸° ì „ëµ** (ë¹„ìš© +$1,613/ì›”)
7. ë…¸ë“œ íƒ€ì… ì—…ê·¸ë ˆì´ë“œ (r7g.large â†’ r7g.xlarge)
8. ìƒ¤ë“œ ìˆ˜ ì¦ê°€ (3 â†’ 5)

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

**í˜„ì¬ ìƒíƒœ** (Pipeline ìµœì í™” ì „):
- Redis CPU: **99.8%**
- Redis Commands: **70,000/sec**
- Gateway ì—ëŸ¬ìœ¨: **~30%** (500 ì—ëŸ¬)

**Pipeline ìµœì í™” í›„** (Week 1):
- Redis CPU: **~30%** (71% ê°ì†Œ)
- Redis Commands: **20,000/sec** (71% ê°ì†Œ)
- Gateway ì—ëŸ¬ìœ¨: **~5%** (Circuit Breaker fallback)

**Full ìµœì í™” í›„** (Week 4):
- Redis CPU: **~15-20%** (ì•ˆì •)
- Redis Commands: **20,000/sec** (5 shardsë¡œ ë¶„ì‚°)
- Gateway ì—ëŸ¬ìœ¨: **< 1%**
- ì˜¤í† ìŠ¤ì¼€ì¼ë§: **ì‚¬ì „ ì˜ˆë°©ì  ëŒ€ì‘**

---

## ğŸ”§ ì‹¤í–‰ ê°€ëŠ¥í•œ Next Steps

1. **ì¦‰ì‹œ ì‹¤í–‰**:
   ```bash
   # Circuit Breaker ë°°í¬
   git add internal/middleware/circuit_breaker.go
   git commit -m "feat: Add Circuit Breaker for Redis resilience"
   git push
   
   # Connection Pool ì„¤ì • ì¡°ì •
   kubectl edit configmap gateway-api-config -n tacos-app
   kubectl rollout restart deployment/gateway-api -n tacos-app
   ```

2. **ì˜¤í† ìŠ¤ì¼€ì¼ë§ ê°œì„ **:
   ```bash
   cd ../traffic-tacos-infra-iac
   # modules/elasticache/main.tf ìˆ˜ì •
   terraform plan
   terraform apply
   ```

3. **ëª¨ë‹ˆí„°ë§ ê°•í™”**:
   ```bash
   # CloudWatch ëŒ€ì‹œë³´ë“œ ì¶”ê°€
   aws cloudwatch put-dashboard --dashboard-name Redis-Health --dashboard-body file://redis-dashboard.json
   ```

---

**ë¬¸ì„œ ì‘ì„±**: 2025-10-08
**ìµœì¢… ìˆ˜ì •**: 2025-10-08
**ì‘ì„±ì**: Traffic Tacos Platform Team

